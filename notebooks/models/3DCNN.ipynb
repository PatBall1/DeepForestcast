{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D CNN explained!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first proposed model, 2DCNN, is able to take advantage of the spatial and spectral domain of the input due to its 2D convolutional layers. However,  it is not able to utilize the time domain of a datapoint $j$. It analyzes a 3D tensor $\\mathbf{SX_t}^j$ that arses from stacking the Static features of $j$, $\\mathbf{S}^j$, and only one element of $j^{th}$ time series $\\mathbf{X}^j_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second model, 3D CNN, on the other side takes full advantage of the time domain and thus is able to analyze $j^{th}$ spectral,spatial and time domain simultaneously. This is achieved via 3D convlutional layers present in its architecture, that consists of two branches which meet together to return the final prediction.\n",
    "\n",
    " More precisely, for the $j^{th}$ datapoint the model takes as an input the $j^s$ static tensor $\\mathbf{S}^j \\in \\mathbb{R}^{2 \\times (2r+1) \\times(2r+1)}$ and its full time series of non static tensors $\\{\\textbf{X}^j_{t-2},\\textbf{X}^j_{t-1},\\textbf{X}^j_{t}\\}$, each $\\in \\mathbb{R}^{k \\times (2r+1) \\times(2r+1)}$, here $k = 8$. It then propagate these two parts of the input to two different branches. The static tensor $\\mathbf{S}^j$ is passed to a 2D Convolutional Branch and the time series of tensors $\\{\\mathbf{X}^j_k\\}^t_{k=t-3}$ to a 3D Convolutional Branch (in the form of 4D tensor of shape (c,t,h,w)). Each branch extract high-level features which are then propagated together in the rest of the network. Finally, the model returns an output $\\hat Y^j_{t+1}$ indicating the confidence of the model to observe deforestation at the target location in the following year t+1 where $j \\in \\mathbf{J}_{t}$ (see Chapter 4 for more details about ths notation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we give more detailed explanation what each branch does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{S}^j$ is passed to a 2D Convolutional branch that \"convolve\" with the input along the spatial domain in two sequential 2D convolutional layers (conv. layers) and return a 3D tensor of high level features, $\\mathbf{Z}^j_s$. These 2 conv. layers have the same type of filters. Each filter sldes at stride = 1, no padding is performed and theirs spatial size is regilated by the first argument of the model parameter kernel_size. Since we want to stack \n",
    "the output of this model branch with this that process $\\mathbf{X}_t$ along the channel axis, we want both branches outputs to have the same spatial sizes. Therefore we set the model to have convolutional filters in both those branches, conv_2D and conv_3D, of the same spatial size. Both 3D tensors of high-level filters evolving from these two branches have the same spatial size, $(h \\times w)$. We summarie this branch as follows:\n",
    "\n",
    "**conv_2D = torch.nn.Sequential** : $ \\mathbf{S}^j \\in \\mathbb{R}^{2 \\times (2r+1) \\times(2r+1)} \\rightarrow \n",
    "2 \\times (2DConv \\rightarrow ReLU \\rightarrow 2DBN) \\rightarrow \\mathbf{Z}^j_{s} \\in \\mathbb{R}^{c_1,h,w} $\n",
    " \n",
    "         self.conv_2D = torch.nn.Sequential(        \n",
    "            torch.nn.Conv2d(input_dim[0],hidden_dim[0],kernel_size = kernel_size[0]), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(hidden_dim[0]),\n",
    "            \n",
    "            torch.nn.Conv2d(hidden_dim[0],hidden_dim[0],kernel_size = kernel_size[0]), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(hidden_dim[0]))\n",
    "            \n",
    "hidden_dim[0] defines the number of filters in each layer of this brach. Hene $c_1$ = hidden_dim[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>$\\{\\mathbf{X}^j_k\\}^t_{k=t-3}$ is passed to a 3D Convolutional Branch in the form of 4D tensor obtained by stacking the sequence by the time domain. Thus the input tensor $\\mathbf{X}^j$ $\\in \\mathbb{R}^{8 \\times 3 \\times (2r+1) \\times(2r+1)}$ has its first domain defined by the channels, the second by the time and the last two, by the space. The 3D Convolutional Branch \"convolve\" with $\\mathbf{X}^j$ across its last three dimensions, time, height and width in two sequential 3D convolutional layers. Due to our limited time domain, of size 3, the 4D filters have shape $\\in \\mathbb{R}^{c,2,k_h,k_w}$, where the first dimension extends to the number of the input channels and the last three define the shape of its 3D kernels. While setting different values to their spatial sizes is possible, the size of its time domain could only be 2. After propagating the input through the 2 3D conv layers, its time domain is decresed to 1 : 3 - k_size[t] + 2*padding)/stride[t] + 1 = 2 and (2 - k_size[t] + 2*padding)/stride[t] + 1 = 1, where k_size[t] = 2, stride[t] = 1, padding = 0. The model slides its filters' 3D kernels along thee time domain at stride 1 and no padding is applied on the input 4D tensor. Therefore, after the two 3D convlutional layers the output of the 3D Convoltional Branch was a 3D tensor of high-level features with no time domain, $\\mathbf{Z}^j_x$. We summarize this propagation as follows:\n",
    "\n",
    "**conv_3D = torch.nn.Sequential** : $\\mathbf{X}^j \\in \\mathbb{R}^{8 \\times 3 \\times (2r+1) \\times(2r+1)} \\rightarrow 2\\times (3DConv \\rightarrow ReLU \\rightarrow 3DBN) \\rightarrow \\mathbf{Z}^j_{x} \\in \\mathbb{R}^{c_2,h,w}$\n",
    "\n",
    "            self.conv_3D = torch.nn.Sequential(\n",
    "                        torch.nn.Conv3d(in_channels = input_dim[1],\n",
    "                                        out_channels = hidden_dim[1],\n",
    "                                        kernel_size = kernel_size[1]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm3d(hidden_dim[1]),\n",
    "            \n",
    "                        torch.nn.Conv3d(in_channels = hidden_dim[1],\n",
    "                                        out_channels = hidden_dim[1],\n",
    "                                        kernel_size = kernel_size[1]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm3d(hidden_dim[1]))    \n",
    "                        \n",
    "As mentioned above, the spatial sizes of the filters in thin brach are the same as in conv_2d brach:\n",
    "kernel_sizes[0] = kernel_sizes[1]. Eg: kernel_size=((3,3),(2,3,3),(5,5)) or kernel_size=((5,5),(2,5,5),(5,5)). Here 2 in (2,5,5) is kentel_size[t].\n",
    "\n",
    "hidden_dim[1] defines the number of filters in each layer of this brach. Hene $c_2$ = hidden_dim[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This two 3D tensors of high-level features,$\\mathbf{Z}^j_x,\\mathbf{Z}^j_s$, returned form each branch are then stacked along their third domain to form the 3D tensor $\\mathbf{Z}^j$.\n",
    "\n",
    "\n",
    "    def forward(self, data , sigmoid = True ):\n",
    "        \n",
    "        s , x = data\n",
    "\n",
    "        s = self.conv_2D.forward(s)\n",
    "        x = self.conv_3D.forward(x)        \n",
    "        \n",
    "as mentioned above, s after conv_2D is of shape (b,c_1,h,w) and x after conv_3D of (b,c_2,1,h,w);\n",
    "Stack them together along the channel axis:\n",
    "\n",
    "        x = x.squeeze(dim = 2 )\n",
    "        x = torch.cat((x,s),dim = 1)\n",
    "        \n",
    "and propagated it to the rest of the network. The final part of the network has another six 2D convolutional layers. \n",
    "In this six 2D convolutional layers all filters have shape kernel_size[2], stride = 1 and no padding. The number of filters in each layer is set via hidden_dim[2]. This propagation is\n",
    "\n",
    "        x = self.final.forward(x) \n",
    "        \n",
    "and after the last convolutional operation, the output is propagated to a SPP layer\n",
    "\n",
    "        x = spp_layer(x, self.levels)\n",
    "        \n",
    "and two FC layers with DO in between and a sigmoid squashing function as in our CNN model:\n",
    "\n",
    "        x= self.ln(x)\n",
    "        if sigmoid: \n",
    "            x = self.sig(x)  \n",
    "\n",
    "Where:\n",
    "\n",
    "        self.final = torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d(hidden_dim[0]+hidden_dim[1], hidden_dim[2], kernel_size[2]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm2d(hidden_dim[2]),\n",
    "            \n",
    "                        torch.nn.Conv2d(hidden_dim[2], hidden_dim[2], kernel_size[2]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm2d(hidden_dim[2]),\n",
    "        \n",
    "                        torch.nn.Conv2d(hidden_dim[2], hidden_dim[2], kernel_size[2]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm2d(hidden_dim[2]),\n",
    "            \n",
    "                        torch.nn.Conv2d(hidden_dim[2], hidden_dim[2], kernel_size[2]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm2d(hidden_dim[2]),\n",
    "            \n",
    "                        torch.nn.Conv2d(hidden_dim[2], hidden_dim[2], kernel_size[2]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm2d(hidden_dim[2]),\n",
    "                    \n",
    "                        torch.nn.Conv2d(hidden_dim[2], hidden_dim[2], kernel_size[2]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm2d(hidden_dim[2]))  \n",
    "And\n",
    "\n",
    "        self.ln = torch.nn.Sequential( \n",
    "            torch.nn.Linear(ln_in,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(100),\n",
    "            torch.nn.Dropout(dropout),           \n",
    "            torch.nn.Linear(100, 1))\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    " We summarise the final part of the network as follows:\n",
    "$\\mathbf{Z}^j \\in \\mathbb{R}^{(c_1+c_2),h,w} \\rightarrow 6 \\times (2DConv \\rightarrow ReLU \\rightarrow 2DBN) \\rightarrow SPP(n,\\mathbf{k}) \\rightarrow FC(spp,100) \\rightarrow ReLU \\rightarrow 1DBN \\rightarrow DO(p) \\rightarrow FC(100,1) \\rightarrow \\sigma \\rightarrow \\hat Y^j_{t+1}$\n",
    "\n",
    "Dropout is regulated by the dropout parameter of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, again, we utilized 2DBN and 3DBN between each 2D and 3D conv. layers after applying ReLU activation function. The number of filters in each of the 2D and 3D conv layers of the 3D_conv , 2D_conv and final branch was set as free parameter, with default set to **hidden_dim**=(16,32,32) for the 2D_conv, 3D_conv  and final branch respectively. The spatial size of the filters was also free parameter, with default set to **kernel_size** = ((5,5),(2,5,5),(5,5)) for the 2D_conv, 3D_conv  and final branch respectively. Parameters of SPP layer and DO layers are allowed to vary too, with default set to **levels** =(13,), **dropout** = 0.2. The model is able to analyze tensors of any spatial size. This model flexability allowed us to experiment with its architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3DConvModel.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/rdsgpfs/general/project/aandedemand/live/satellite/junin/deforestation_forecasting/python_code/Notebooks'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rdsgpfs/general/project/aandedemand/live/satellite/junin/deforestation_forecasting/python_code\n"
     ]
    }
   ],
   "source": [
    "%cd \"/rdsgpfs/general/project/aandedemand/live/satellite/junin/deforestation_forecasting/python_code\"\n",
    "import torch\n",
    "from spp_layer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_3D(torch.nn.Module):\n",
    "    def __init__(self, input_dim=(2,8),\n",
    "                 hidden_dim=(16,32,32),\n",
    "                 kernel_size=((5,5),(2,5,5),(5,5)),\n",
    "                 levels=(10,),\n",
    "                 dropout = 0.2):\n",
    "        super(Conv_3D, self).__init__()\n",
    "        \n",
    "        self.levels = levels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.conv_2D = torch.nn.Sequential(        \n",
    "            torch.nn.Conv2d(input_dim[0],hidden_dim[0],kernel_size = kernel_size[0]), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(hidden_dim[0]),\n",
    "            \n",
    "            torch.nn.Conv2d(hidden_dim[0],hidden_dim[0],kernel_size = kernel_size[0]), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(hidden_dim[0]))   \n",
    "        \n",
    "        self.conv_3D = torch.nn.Sequential(\n",
    "                        torch.nn.Conv3d(in_channels = input_dim[1],\n",
    "                                        out_channels = hidden_dim[1],\n",
    "                                        kernel_size = kernel_size[1]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm3d(hidden_dim[1]),\n",
    "            \n",
    "                        torch.nn.Conv3d(in_channels = hidden_dim[1],\n",
    "                                        out_channels = hidden_dim[1],\n",
    "                                        kernel_size = kernel_size[1]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm3d(hidden_dim[1]))    \n",
    "        \n",
    "        self.final = torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d(hidden_dim[0]+hidden_dim[1], hidden_dim[2], kernel_size[2]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm2d(hidden_dim[2]),\n",
    "            \n",
    "                        torch.nn.Conv2d(hidden_dim[2], hidden_dim[2], kernel_size[2]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm2d(hidden_dim[2]),\n",
    "        \n",
    "                        torch.nn.Conv2d(hidden_dim[2], hidden_dim[2], kernel_size[2]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm2d(hidden_dim[2]),\n",
    "            \n",
    "                        torch.nn.Conv2d(hidden_dim[2], hidden_dim[2], kernel_size[2]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm2d(hidden_dim[2]),\n",
    "            \n",
    "                        torch.nn.Conv2d(hidden_dim[2], hidden_dim[2], kernel_size[2]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm2d(hidden_dim[2]),\n",
    "                    \n",
    "                        torch.nn.Conv2d(hidden_dim[2], hidden_dim[2], kernel_size[2]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.BatchNorm2d(hidden_dim[2]))  \n",
    "        \n",
    "        ln_in = 0\n",
    "        for i in levels:\n",
    "            ln_in += hidden_dim[2]*i*i\n",
    "        \n",
    "        self.ln = torch.nn.Sequential( \n",
    "            torch.nn.Linear(ln_in,100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(100),\n",
    "            torch.nn.Dropout(dropout),           \n",
    "            torch.nn.Linear(100, 1))\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, data , sigmoid = True ):\n",
    "        \n",
    "        s , x = data\n",
    "\n",
    "        s = self.conv_2D.forward(s)\n",
    "        x = self.conv_3D.forward(x)        \n",
    "        x = x.squeeze(dim = 2 )\n",
    "        x = torch.cat((x,s),dim = 1)\n",
    "        x = self.final.forward(x) \n",
    "        x = spp_layer(x, self.levels)\n",
    "        x= self.ln(x)\n",
    "        if sigmoid: \n",
    "            x = self.sig(x)  \n",
    "            \n",
    "        return x.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again one must be careful when setting the levels and the kernel_sizes of the model. Below we illustrate how the spatial size of the input decreases until it is passed to spp_layer when defaut parameters are used.\n",
    "\n",
    "We chose those parameters, as they are very similar to the 2D CNN model that showed good results.\n",
    "levels is set so that when the input image is of size 45, no pooling is applied, and any input greater than 45 will be downsampled.\n",
    "\n",
    "A possible modification of this model will be to have deeper conv_2d branch where the only requirement is to have output spatial size matching to the 3d_conv branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set image parameters\n",
    "size = 45\n",
    "#set model parameters for 3D_CNN\n",
    "input_dim= (2,8)\n",
    "hidden_dim=(16,32,32)\n",
    "kernel_size=((5,5),(2,5,5),(5,5))\n",
    "levels=(13,)\n",
    "dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv_3D(\n",
    "    input_dim = input_dim,\n",
    "    hidden_dim = hidden_dim,\n",
    "    kernel_size= kernel_size,\n",
    "    levels=levels,\n",
    "    dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5335, 0.4771], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, c1, c2, t, h, w = 3, 2, 8, 3, size, size\n",
    "s = torch.rand(2,c1,size,size)\n",
    "x = torch.rand(2,c2,t,size,size)\n",
    "data = (s,x)\n",
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image spatial size: (45, 45)\n",
      "Changes of the spatial size in the two branches (2D_cov and 3D_conv)\n",
      "\tLayer 1\n",
      "\tkernel_size:  (5, 5)\n",
      "\tSize after layer 1 is applied: [41 41]\n",
      "\n",
      "\tLayer 2\n",
      "\tkernel_size:  (5, 5)\n",
      "\tSize after layer 2 is applied: [37 37]\n",
      "\n",
      "Changes of the spatial size in the final brach:\n",
      "\tLayer 1\n",
      "\tkernel_size:  (5, 5)\n",
      "\tSize after layer 1 is applied: [33 33]\n",
      "\n",
      "\tLayer 2\n",
      "\tkernel_size:  (5, 5)\n",
      "\tSize after layer 2 is applied: [29 29]\n",
      "\n",
      "\tLayer 3\n",
      "\tkernel_size:  (5, 5)\n",
      "\tSize after layer 3 is applied: [25 25]\n",
      "\n",
      "\tLayer 4\n",
      "\tkernel_size:  (5, 5)\n",
      "\tSize after layer 4 is applied: [21 21]\n",
      "\n",
      "\tLayer 5\n",
      "\tkernel_size:  (5, 5)\n",
      "\tSize after layer 5 is applied: [17 17]\n",
      "\n",
      "\tLayer 6\n",
      "\tkernel_size:  (5, 5)\n",
      "\tSize after layer 6 is applied: [13 13]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_input_size(size, kernel_size):\n",
    "    print(\"Input image spatial size:\",size)\n",
    "    print(\"Changes of the spatial size in the two branches (2D_cov and 3D_conv)\")\n",
    "    for i in range(2):\n",
    "        print(\"\\tLayer\",i+1)\n",
    "        print(\"\\tkernel_size: \", kernel_size[0])\n",
    "        size = np.array(size) - np.array(kernel_size[0]) + 1\n",
    "        print(\"\\tSize after layer %d is applied:\"%(i+1), size)\n",
    "        print()\n",
    "    print(\"Changes of the spatial size in the final brach:\")\n",
    "    for i in range(6):\n",
    "        print(\"\\tLayer\",i+1)\n",
    "        print(\"\\tkernel_size: \", kernel_size[2])\n",
    "        size = np.array(size) - np.array(kernel_size[2])  + 1\n",
    "        print(\"\\tSize after layer %d is applied:\"%(i+1), size)\n",
    "        print()\n",
    "\n",
    "check_input_size((size,size), kernel_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 with PyTorch",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
